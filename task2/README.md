# Задание 2
Реализация умножения матрицы на вектор, используя разбиение по строкам, столбцам и блокам.
## Разбиение по строкам
### Принцип работы
Матрица распределяется между процессами построчно. Каждый процесс получает несколько целых строк исходной матрицы и вычисляет соответствующие элементы результирующего вектора.
### Описание реализации
Алгоритм работает путем распределения строк матрицы между процессами. Сначала процесс 0 читает размеры матрицы и создает исходные данные. Затем размеры рассылаются всем процессам через MPI_Bcast(). На процессе 0 вычисляются параметры распределения: базовое количество строк и остаток. Эти параметры используются для создания массивов sendcounts, displs, recvcounts и recvdispls, которые определяют, сколько данных и где получит каждый процесс.

После этого каждый процесс узнает количество строк, которое он получит, через MPI_Scatter(). Все процессы выделяют локальную память для своей части матрицы и результата. Матрица распределяется через MPI_Scatterv() с неравномерным разбиением, а вектор рассылается всем через MPI_Bcast().

Перед вычислениями все процессы синхронизируются через MPI_Barrier(), после чего начинается измерение времени. Каждый процесс выполняет функцию multRows(), которая вычисляет скалярные произведения для локальных строк. После завершения вычислений собираются результаты через MPI_Gatherv() в процесс 0. Максимальное время получается через MPI_Reduce() с операцией MPI_MAX.
### Оценка времени работы
<img width="797" height="396" alt="image_2025-11-07_00-23-26" src="https://github.com/user-attachments/assets/d862e74d-a119-40f9-a1db-634c4ff51b08" />
<img width="798" height="394" alt="image_2025-11-07_00-25-22" src="https://github.com/user-attachments/assets/d7a65bf5-296b-4e49-9acd-835af855f694" />
<img width="801" height="393" alt="image_2025-11-07_00-28-13" src="https://github.com/user-attachments/assets/68e93c50-7bdf-45d1-aebe-65af0833c117" />

## Разбиение по столбцам
### Принцип работы
Каждый процесс получает определенный диапазон столбцов матрицы и все элементы вектора. Каждый процесс вычисляет частичный результирующий вектор путем умножения своего диапазона столбцов на соответствующие элементы вектора.
### Описание реализации
При этом подходе процесс 0 читает размеры и создает данные. После рассылки размеров каждый процесс != 0 выделяет память для полной матрицы и вектора. Здесь используется MPI_Bcast(), который отправляет всю матрицу и весь вектор каждому процессу.

Каждый процесс вычисляет свой диапазон столбцов. Базовое количество столбцов — это cols / comm_sz, а остаток распределяется между первыми процессами. Процессы с меньшим рангом получают на один столбец больше. Каждый процесс выделяет локальный результат размером rows и инициализирует его нулями.

После синхронизации каждый процесс выполняет функцию multCols(), обрабатывая только свой диапазон столбцов исходной (полной) матрицы. Результаты вычисляются как частичные векторы. Затем все частичные результаты суммируются в процессе 0 через MPI_Reduce() с операцией MPI_SUM. Максимальное время вычисляется аналогично.
### Оценка времени работы
<img width="797" height="396" alt="image_2025-11-07_00-30-15" src="https://github.com/user-attachments/assets/45e4b59b-22df-4a75-a311-a4b8e7017bce" />
<img width="796" height="392" alt="image_2025-11-07_00-33-10" src="https://github.com/user-attachments/assets/36a0e3af-10e1-4525-96e8-8a4bff64256d" />
<img width="805" height="388" alt="image_2025-11-07_00-40-46" src="https://github.com/user-attachments/assets/7ff13443-0aff-42ea-a2a3-40f21035a3be" />


## Разбиение по блокам
### Принцип работы
Процессы организуются в двумерную квадратную сетку (требуется количество процессов, равное полному квадрату). Каждый процесс получает свою позицию в сетке и определяет прямоугольный блок матрицы, который он будет обрабатывать. Каждый процесс вычисляет частичный результирующий вектор путем умножения своего прямоугольного блока на соответствующие элементы вектора.
### Описание реализации
Этот алгоритм требует, чтобы количество процессов было полным квадратом (4, 9, 16, 25...). Процесс 0 читает размеры и создает данные. Далее вычисляется размер логической 2D сетки: block = sqrt(comm_sz) и количество активных процессов blocksNum = block * block.

Каждый процесс определяет свою позицию в сетке через формулы: my_row = rank / block и my_col = rank % block. Границы блока вычисляются как произведение позиции на размер блока. Для граничных блоков проводится коррекция, чтобы охватить остаток матрицы.

Как и в столбцовом алгоритме, используется MPI_Bcast() для рассылки полной матрицы и вектора. После синхронизации каждый процесс выполняет функцию multBlocks() для своего прямоугольного блока матрицы. Результаты суммируются через MPI_Reduce() с MPI_SUM.
### Оценка времени работы
<img width="800" height="382" alt="image_2025-11-07_00-42-46" src="https://github.com/user-attachments/assets/0f11feaa-8434-4354-a518-f6e0d2bd0d4c" />
<img width="801" height="394" alt="image_2025-11-07_00-44-45" src="https://github.com/user-attachments/assets/b181b100-c1f8-4905-92d7-0d19f8f9b7b1" />
<img width="801" height="388" alt="image_2025-11-07_00-46-28" src="https://github.com/user-attachments/assets/46088375-53c5-4512-91a6-3c9d61ca7163" />


